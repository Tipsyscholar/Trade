{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":9963315,"sourceType":"datasetVersion","datasetId":6128728}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#from pytorch_tabnet.tab_model import TabNetRegressor\nimport warnings\nimport os\n#environment provided by competition hoster\nimport kaggle_evaluation.mcts_inference_server\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport plotly.graph_objects as go\npd.options.display.max_rows = None\npd.options.display.max_columns = None\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error as mse\nimport matplotlib.pyplot as plt\nimport pyarrow as pa\nplt.rcParams['axes.unicode_minus'] = False\nimport json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FE:\n    \n    def __init__(self, batch_size):\n        self.batch_size = batch_size\n        self.bad_cols=[]\n    \n    def agent_trans(self, df):\n        # 字典定义\n        agent_dict = {\n            '1': ['UCB1', 'UCB1Tuned', 'UCB1GRAVE', 'ProgressiveHistory'],\n            '2': ['0.1', '0.6', '1.41421356237'],\n            '3': ['Random200', 'MAST', 'NST'],\n            '4': ['true']\n        }\n        \n        # 获取所有非数值类型的列（即分类列）\n        cat_cols = df.select_dtypes(exclude=['number']).columns\n        \n        # 遍历字典进行转换\n        for i in range(1,3):\n            for key, value in agent_dict.items():\n                for j in value:\n                    # 将布尔值转化为相应的列\n                    df[f'agent{i}_split{key}_{j}'] = df[f'agent{i}'].apply(lambda x: x.split('-')[int(key)] == j)\n        \n        # 将所有布尔类型的列转换为 0 和 1\n        df[df.select_dtypes(include=['bool']).columns] = df.select_dtypes(include=['bool']).astype(int)\n        \n        # 针对非分类列进行类型转换\n        for col in df.columns:\n            if col not in cat_cols:  # 排除分类列\n                # 删除NaN并检查是否为空\n                cleaned_col = df[col].dropna()\n                if not cleaned_col.empty:\n                    val = cleaned_col.iloc[0]\n                    df[col] = df[col].astype('int16') if isinstance(val, int) else df[col].astype('float32')\n                else:\n                    # 如果列为空，可以选择设置为默认值或跳过\n                    df[col] = df[col].astype('float32')  # 或者你可以设置为 int16，取决于需要\n\n        return df\n    \n    def drop_cols(self, df, bad_cols=None):\n        # 多余特征列\n        cols = ['Id', \n                'LudRules', \n                'EnglishRules',\n                'num_wins_agent1',\n                'num_draws_agent1',\n                'num_losses_agent1',\n                'agent1',\n                'agent2'\n               ]\n        \n        # 将多余的列添加到 bad_cols 中\n        self.bad_cols.extend([col for col in cols if col in df.columns])\n        \n        # 删除多余的列\n        df = df.drop(columns=[col for col in cols if col in df.columns])\n        \n        # 删除全为 NaN 的列，并将它们添加到 bad_cols\n        nan_cols = [col for col in df.columns if df[col].isna().all()]\n        self.bad_cols.extend(nan_cols)  # 记录全为 NaN 的列\n        df = df.dropna(axis=1, how='all')\n        \n        # 删除值为单一的列，并将其添加到 bad_cols\n        if bad_cols is None:\n            bad_cols = [col for col in df.columns if df[col].nunique() == 1]\n        df = df.drop(columns=bad_cols)\n        \n        # 将删除的单一值列添加到 bad_cols\n        self.bad_cols.extend(bad_cols)\n        \n        return df\n\n    \n    def info(self, df):\n        print(f'Shape: {df.shape}')   \n        mem = df.memory_usage(deep=True).sum() / 1024**2\n        print(f'Memory usage: {mem:.2f} MB\\n')\n        \n    def apply_fe(self, path):\n        # 读取数据\n        df = pd.read_csv(path)\n        \n        # 执行数据处理\n        df = self.agent_trans(df)\n        df = self.drop_cols(df)\n        self.info(df)\n        \n        return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    \n    train_path = Path('/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv')\n    batch_size = 65536\n\n    early_stop = 100\n    n_features = 200\n    n_splits = 5\n    color = '#C9A9A6'\n    \n    lgb_p = {\n        'objective': 'regression',\n        'min_child_samples': 24,\n        'num_iterations': 500,\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 0.8,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'metric': 'rmse',\n        'device': 'cpu',\n        'max_depth': 8,\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42\n    }\n    \n    ctb_p = {\n        'loss_function': 'RMSE',\n        'learning_rate': 0.03,\n        'num_trees': 500,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'reg_lambda': 0.8,\n        'depth': 8\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MD:\n    \n    def __init__(self, early_stop, n_features, n_splits, lgb_p, ctb_p, color):\n        self.early_stop = early_stop\n        self.n_features = n_features\n        self.n_splits = n_splits\n        self.lgb_p = lgb_p\n        self.ctb_p = ctb_p\n        self.color = color\n        \n    def plot_cv(self, fold_scores, title):\n        \n        fold_scores = [round(score, 3) for score in fold_scores]\n        mean_score = round(np.mean(fold_scores), 3)\n        std_score = round(np.std(fold_scores), 3)\n\n        fig = go.Figure()\n\n        fig.add_trace(go.Scatter(\n            x = list(range(1, len(fold_scores) + 1)),\n            y = fold_scores,\n            mode = 'markers', \n            name = 'Fold Scores',\n            marker = dict(size = 24, color=self.color, symbol='diamond'),\n            text = [f'{score:.3f}' for score in fold_scores],\n            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n            hoverlabel=dict(font=dict(size=16))  \n        ))\n\n        fig.add_trace(go.Scatter(\n            x = [1, len(fold_scores)],\n            y = [mean_score, mean_score],\n            mode = 'lines',\n            name = f'Mean: {mean_score:.3f}',\n            line = dict(dash = 'dash', color = '#FFBF00'),\n            hoverinfo = 'none'\n        ))\n\n        fig.update_layout(\n            title = f'{title} | Cross-Validation RMSE Scores | Variation of CV scores: {mean_score} ± {std_score}',\n            xaxis_title = 'Fold',\n            yaxis_title = 'RMSE Score',\n            plot_bgcolor = 'rgba(0,0,0,0)',\n            paper_bgcolor = 'rgba(0,0,0,0)',\n            xaxis = dict(\n                gridcolor = 'lightgray',\n                tickmode = 'linear',\n                tick0 = 1,\n                dtick = 1,\n                range = [0.5, len(fold_scores) + 0.5]\n            ),\n            yaxis = dict(gridcolor = 'lightgray')\n        )\n\n        fig.show() \n        \n    def train_model(self, data, title):\n        # Define features (X), label (y) and grouping column (group) for CV\n        X = data.drop(['utility_agent1'], axis=1)\n        y = data['utility_agent1']\n        group = data['GameRulesetName']\n        \n        # Convert 'GameRulesetName' and other categorical columns to 'category'\n        X['GameRulesetName'] = X['GameRulesetName'].astype('category')\n        \n        # If you have other categorical columns, convert them similarly:\n        cat_cols = X.select_dtypes(include=['object']).columns\n        for col in cat_cols:\n            X[col] = X[col].astype('category')\n    \n        cv = GroupKFold(n_splits=self.n_splits)\n        \n        models, scores = [], []\n        \n        # Initialize out-of-fold predictions array\n        oof_preds = np.zeros(len(X))\n        \n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y, group)):\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            categorical_cols = ['GameRulesetName']\n            if title.startswith('LightGBM'):\n                model = lgb.LGBMRegressor(**self.lgb_p)\n    \n                # Pass categorical columns\n                  # Add more categorical columns if necessary\n                model.fit(X_train, y_train,\n                          eval_set=[(X_valid, y_valid)],\n                          eval_metric='rmse',\n                          categorical_feature=categorical_cols,\n                          callbacks=[lgb.early_stopping(self.early_stop, verbose=0), lgb.log_evaluation(0)])\n                \n            elif title.startswith('CatBoost'):\n                model = CatBoostRegressor(**self.ctb_p, verbose=0,cat_features=categorical_cols)\n                model.fit(X_train, y_train,\n                          eval_set=(X_valid, y_valid),\n                          early_stopping_rounds=self.early_stop, verbose=0)\n    \n            models.append(model)\n    \n            # Store out-of-fold predictions for this fold\n            oof_preds[valid_index] = model.predict(X_valid)\n            score = mse(y_valid, oof_preds[valid_index], squared=False)\n            scores.append(score)\n        \n        self.plot_cv(scores, title)\n        \n        return models\n\n    \n    def feature_importance(self, data, title):\n\n        \n        models = self.train_model(data, title)\n        \n        feature_cols = [col for col in data.columns if col != 'utility_agent1']\n        \n        feature_importances = np.zeros(len(feature_cols))\n        for model in models:\n                \n            if title.startswith('LightGBM'):\n                feature_importances += model.feature_importances_ / len(models)\n            \n            elif title.startswith('CatBoost'):\n                feature_importances += model.get_feature_importance() / len(models)\n        \n        feature_importance = pd.DataFrame({\n            'feature': feature_cols,\n            'importance': feature_importances\n        })\n        \n        feature_importance = feature_importance.sort_values('importance', ascending=False).reset_index(drop=True)\n        \n        drop_features = feature_importance.loc[self.n_features:, 'feature'].tolist()\n                    \n        return drop_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fe = FE(CFG.batch_size)\nmd = MD(CFG.early_stop, CFG.n_features, CFG.n_splits, CFG.lgb_p, CFG.ctb_p, CFG.color)\ntrain = fe.apply_fe(CFG.train_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nchunksize = 10**4\noutput_file = 'number_clean_feature.csv'\n\ntrain.to_csv(output_file, mode='w', header=True, index=False, chunksize=chunksize)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_lgb_features = md.feature_importance(train,'LightGBM')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md = MD(CFG.early_stop, CFG.n_features, CFG.n_splits, CFG.lgb_p, CFG.ctb_p, CFG.color)\ndrop_ctb_features = md.feature_importance(train, 'CatBoost')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_features = list(set(drop_lgb_features) & set(drop_ctb_features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#importances = pd.DataFrame({\n#    'drop_features': drop_features\n#})\n\n#print(f'Shape: {importances.shape}')\n#display(importances)\n#importances.to_csv('importances.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import LinearRegression\n\n# LGBM模型定义\nclass LGBM_Model:\n    def __init__(self):\n        self.model = None\n        self.params = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'max_depth': -1,\n            'n_estimators': 5,\n            'early_stopping_rounds': 40\n        }\n\n    def train(self, X_train, y_train, X_valid, y_valid):\n        self.model = lgb.LGBMRegressor(**self.params)\n        self.model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n\n    def predict(self, X):\n        if self.model is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        return self.model.predict(X)\n\n# CatBoost模型定义\nclass CatBoost_Model:\n    def __init__(self):\n        self.model = None\n        self.params = {\n            'iterations': 5,\n            'learning_rate': 0.05,\n            'depth': 6,\n            'loss_function': 'RMSE',\n            'early_stopping_rounds': 40,\n            'cat_features': []\n        }\n\n    def train(self, X_train, y_train, X_valid, y_valid):\n        self.model = CatBoostRegressor(**self.params)\n        self.model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n\n    def predict(self, X):\n        if self.model is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        return self.model.predict(X)\n\n# XGBoost模型定义\nclass XGB_Model:\n    def __init__(self):\n        self.model = None\n        self.params = {\n            'objective': 'reg:squarederror',\n            'learning_rate': 0.05,\n            'max_depth': 6,\n            'n_estimators': 5,\n            'early_stopping_rounds': 40\n        }\n\n    def train(self, X_train, y_train, X_valid, y_valid):\n        self.model = XGBRegressor(**self.params)\n        self.model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n\n    def predict(self, X):\n        if self.model is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        return self.model.predict(X)\n\n# NeuMF模型定义\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass NeuMF_Model:\n    def __init__(self, input_dim, hidden_dim=4):\n        self.model = None\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = 1  # 回归问题\n\n    def build_model(self):\n        model = nn.Sequential(\n            nn.Linear(self.input_dim, self.hidden_dim),\n            nn.ReLU(),\n            nn.Linear(self.hidden_dim, self.output_dim)\n        )\n        return model\n\n    def train(self, X_train, y_train, X_valid, y_valid):\n        # Check if GPU is available, otherwise use CPU\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Build and move the model to the device (GPU or CPU)\n        self.model = self.build_model().to(device)\n        \n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n\n        # Move data to the device\n        train_data = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n        train_labels = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n        valid_data = torch.tensor(X_valid.values, dtype=torch.float32).to(device)\n        valid_labels = torch.tensor(y_valid.values, dtype=torch.float32).to(device)\n\n        epochs = 5\n        for epoch in range(epochs):\n            self.model.train()\n            optimizer.zero_grad()\n            predictions = self.model(train_data)\n            loss = criterion(predictions, train_labels.view(-1, 1))\n            loss.backward()\n            optimizer.step()\n\n            if (epoch + 1) % 10 == 0:\n                self.model.eval()\n                val_predictions = self.model(valid_data)\n                val_loss = criterion(val_predictions, valid_labels.view(-1, 1))\n                print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n\n    def predict(self, X):\n        if self.model is None:\n            raise ValueError(\"Model has not been trained yet.\")\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.eval()\n        data = torch.tensor(X.values, dtype=torch.float32).to(device)\n        with torch.no_grad():\n            predictions = self.model(data)\n        return predictions.cpu().numpy().flatten()  # Return to CPU if needed\n\n# Stacking模型定义\nclass Stacking_Model:\n    def __init__(self, models, meta_model=None):\n        self.models = models\n        self.meta_model = meta_model if meta_model else LinearRegression()\n        self.trained_models = []  # 用于保存训练后的最终子模型\n    \n    def train(self, X, y, groups, drop_features=None, n_splits=2):\n        cv = GroupKFold(n_splits=n_splits)\n        oof_preds = np.zeros(len(X))\n        all_meta_features = []\n    \n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups)):\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n            # 删除不需要的特征\n            if drop_features:\n                X_train = X_train.drop(columns=drop_features, errors='ignore')\n                X_valid = X_valid.drop(columns=drop_features, errors='ignore')\n    \n            meta_features_train = []\n            for model in self.models:\n                # 创建新的模型实例\n                model_instance = model(input_dim=X_train.shape[1]) if model == NeuMF_Model else model()\n                \n                print(f\"Training {model.__name__}...\")\n                model_instance.train(X_train, y_train, X_valid, y_valid)\n                \n                # 只保存最后一个折的模型\n                if fold == n_splits - 1:\n                    self.trained_models.append(model_instance)  # 仅保存最后折的模型实例\n                \n                meta_features_train.append(model_instance.predict(X_valid))\n    \n            # 将不同模型的预测结果合并，确保特征数一致\n            meta_features_train = np.column_stack(meta_features_train)\n            all_meta_features.append(meta_features_train)\n            oof_preds[valid_index] = np.mean(meta_features_train, axis=1)\n    \n        # 训练元学习器\n        meta_features_train = np.concatenate(all_meta_features, axis=0)\n        self.meta_model.fit(meta_features_train, y)\n        print(\"Stacking model training completed.\")\n        return oof_preds\n    \n    def predict(self, X):\n        meta_features_test = []\n        for model_instance in self.trained_models:  # 只使用最终的训练好的子模型\n            print(f\"Predicting with {model_instance.__class__.__name__}...\")\n            preds = model_instance.predict(X)\n            print(preds.shape)\n            meta_features_test.append(preds)\n    \n        # 将不同模型的预测结果合并，确保特征数一致\n        meta_features_test = np.column_stack(meta_features_test)\n        print(\"Prediction completed.\")\n        return self.meta_model.predict(meta_features_test)\n\n\n# 全局模型初始化\ndata=train\nX = data.drop(['utility_agent1','GameRulesetName'], axis=1)\ny = data['utility_agent1']\ngroup = data['GameRulesetName']\n\n\n# 全局模型初始化\nmodels = [LGBM_Model, CatBoost_Model, XGB_Model, NeuMF_Model]  # 包含模型类，而不是实例\nmodel = Stacking_Model(models)  # 初始化堆叠模型\n\ncounter = 0\n\ndef predict(test, submission):\n    global counter\n    global drop_features\n    if counter == 0:\n        # 训练模型，如果已经训练过则不再训练\n        global model\n        global drop_features\n        model.train(X, y, group, drop_features=drop_features, n_splits=2)  # 训练堆叠模型\n        counter += 1  # 训练完后计数器增加，避免重复训练\n\n    # Use the trained model to predict on the test data\n    test= test.to_pandas()\n    test=fe.agent_trans(test)\n    test=test.drop(columns=[col for col in fe.bad_cols if col in test.columns],axis=1)\n    test.drop(['GameRulesetName'],axis=1,inplace=True)\n    test.drop(drop_features,axis=1,inplace=True)\n    predictions = model.predict(test)  # 使用训练好的堆叠模型预测\n\n    # Create a Polars Series for the final submission\n    return submission.with_columns(pl.Series('utility_agent1', predictions))\n\n\n    \ninference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    # For local testing\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',  # Replace with the actual test file path\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'  # Replace with the actual submission file path\n        )\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}